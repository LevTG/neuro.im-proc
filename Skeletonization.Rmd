---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region toc=true -->
<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Параметры-для-запуска" data-toc-modified-id="Параметры-для-запуска-1">Параметры для запуска</a></span></li><li><span><a href="#Считывание-изображения" data-toc-modified-id="Считывание-изображения-2">Считывание изображения</a></span></li><li><span><a href="#Фильтрация-изображения" data-toc-modified-id="Фильтрация-изображения-3">Фильтрация изображения</a></span></li><li><span><a href="#Выделение-сомы" data-toc-modified-id="Выделение-сомы-4">Выделение сомы</a></span></li><li><span><a href="#Определение-центра" data-toc-modified-id="Определение-центра-5">Определение центра</a></span></li><li><span><a href="#Выделение-толстых-отростков" data-toc-modified-id="Выделение-толстых-отростков-6">Выделение толстых отростков</a></span></li><li><span><a href="#Матрица-Гессе" data-toc-modified-id="Матрица-Гессе-7">Матрица Гессе</a></span></li><li><span><a href="#Построение-графа" data-toc-modified-id="Построение-графа-8">Построение графа</a></span><ul class="toc-item"><li><span><a href="#Добавление-точек-сомы-в-граф" data-toc-modified-id="Добавление-точек-сомы-в-граф-8.1">Добавление точек сомы в граф</a></span></li></ul></li><li><span><a href="#Расчет-путей" data-toc-modified-id="Расчет-путей-9">Расчет путей</a></span></li><li><span><a href="#Расчет-встречаемости" data-toc-modified-id="Расчет-встречаемости-10">Расчет встречаемости</a></span></li><li><span><a href="#Фильтрация-точек-по-встречаемости" data-toc-modified-id="Фильтрация-точек-по-встречаемости-11">Фильтрация точек по встречаемости</a></span></li><li><span><a href="#Выделение-тонких-веток" data-toc-modified-id="Выделение-тонких-веток-12">Выделение тонких веток</a></span></li><li><span><a href="#Объединение" data-toc-modified-id="Объединение-13">Объединение</a></span><ul class="toc-item"><li><span><a href="#Выделение-крайних-точек" data-toc-modified-id="Выделение-крайних-точек-13.1">Выделение крайних точек</a></span></li><li><span><a href="#Ближайшие-точки-скелета" data-toc-modified-id="Ближайшие-точки-скелета-13.2">Ближайшие точки скелета</a></span></li><li><span><a href="#Добавление-соединений" data-toc-modified-id="Добавление-соединений-13.3">Добавление соединений</a></span></li></ul></li><li><span><a href="#Многомасштабный-гессиан" data-toc-modified-id="Многомасштабный-гессиан-14">Многомасштабный гессиан</a></span></li></ul></div>
<!-- #endregion -->

# **Next actions and TODOs**
 - [ ] Test performance on other cells
 - [ ] Test performace of the approach with more sigma steps (log scale is preferred, i.e. `2.0**np.arange(-1,5,0.5)`)
 - [ ] Think about a way to regularize vector orientations, using orientations of the neighbours, or at different scales
 - [-] Find a best way to skeletonize the qstack-based arrays and masks (as one of the approaches)
 - [X] Find a way to "glue" together paths, that a close-by and have a similar direction
 - [ ] Visualize different sub-trees in the merged paths (add individually to napari?)
 - [ ] add way to gradually strip/simplify (sub-)graphs for better visualization
 

```{python}
import os
import sys
```

```{python}
# %matplotlib inline

import matplotlib.pyplot as plt
```

```{python}
import cv2
```

```{python}
#import cv2
import scipy
from scipy import ndimage as ndi
import numpy as np
import networkx as nx

from pathlib import Path
```

```{python}
import napari
```

```{python}
from tqdm.auto import tqdm
```

```{python}
import ccdb
import astromorpho as astro
```
```{python}
from networx2napari import draw_edges, draw_nodes
```
```{python}
def eu_dist(p1, p2):
    return np.sqrt(np.sum([(x - y)**2 for x, y in zip(p1, p2)]))
```



 

```{python}
def weight_of_arr(a, b):
    dprod = np.einsum('...ij,...ij->...i', a, b)
    norm_a = np.linalg.norm(a, axis=-1)
    norm_b = np.linalg.norm(b, axis=-1)
    cos_dist = np.nan_to_num(dprod / ((norm_a * norm_b)), nan=0)
    cos_dist = 1 - cos_dist
    return np.abs(1 - cos_dist) * np.mean([norm_a, norm_b], axis=0)
```

```{python}
def calc_edges(arr1, arr2, index1, index2):
    weights = weight_of_arr(arr1, arr2)
    weight = weights.ravel()
    li = threshold_li(weight)
    idx1 = [tuple(i) for i in index1.reshape((-1, index1.shape[-1]))[weight>li]]
    idx2 = [tuple(i) for i in index2.reshape((-1, index2.shape[-1]))[weight>li]]
    return zip(idx1, idx2, 1 - weight[weight>li])
```

```{python}
from collections import defaultdict

def count_points_paths(paths):
    acc = defaultdict(int)
    for path in paths:
        for n in path:
            acc[n] += 1
    return acc
```

```{python}
def get_shell_mask(mask, do_skeletonize=False, as_points=False):
    out = ndi.binary_erosion(mask)^mask
    if do_skeletonize:
        out = skeletonize(out)
    if as_points:
        out = astro.morpho.mask2points(out)
    return out 
```

```{python}
from skimage.filters import threshold_li, threshold_minimum
from skimage.morphology import remove_small_objects
```

```{python}
# def remove_small_objects(mask, min_size=25):
#     labels, nlab = ndi.label(mask)
#     objs = ndi.find_objects(labels)
#     out_mask = np.zeros_like(mask)
#     for k,o in enumerate(objs):
#         cond = labels[o]==(k+1)
#         if np.sum(cond) >= min_size:
#             out_mask[o][cond] = True
#     return out_mask

def largest_region(mask):
    labels, nlab = ndi.label(mask)
    if nlab > 0:
        objs = ndi.find_objects(labels)
        sizes = [np.sum(labels[o]==k+1) for k,o in enumerate(objs)]
        k = np.argmax(sizes)
        return labels==k+1
    else:
        return mask
        
def crop_image(img, mask=None, margin=0, min_obj_size=0):
    if mask is None:
        mask = img > 0
    if min_obj_size > 0:
        mask = remove_small_objects(mask, min_obj_size)
    if margin > 0:
        mask = ndi.binary_dilation(mask, iterations=margin)
    objs = ndi.find_objects(mask)
    min_bnds = np.min([[sl.start for sl in o] for o in objs],0)
    max_bnds = np.max([[sl.stop for sl in o] for o in objs],0)
    crop = tuple(slice(mn,mx) for mn,mx in zip(min_bnds, max_bnds))
    return img[crop]
```

```{python}
plt.rc('figure', dpi=150)
```

# Параметры для запуска

```{python}

```

```{python tags=c("parameters")}
#filename = '3wk-both1-grn-raw.pic'
#data_dir = '/home/levtg/astro-morpho/data/'
data_dir = '/home/brazhe/yadisk/data-shared-comfi/3D-astrocyte-images/selected-for-complexity/'
filename = '3wk-both1-grn-raw.pic'

use_clahe = True

verbose = True
sigma = 2

# Set false to start from console
HANDY = True

# Set true to save output
OUT = False
```

```{python}
filename = Path(data_dir).joinpath(filename)
filename
```

# Считывание изображения

```{python}
# if HANDY:
#     filename = '/home/levtg/astro-morpho/data/3wk-ly10-raw.pic'
```

```{python}
stack, meta = ccdb.read_pic(filename)
dims = ccdb.get_axes(meta)
dims
```

```{python}
if len(dims):
    zoom = (dims[-1][0]/dims[0][0])
else:
    zoom = 4
    
print(zoom)
```

```{python}

```

### clahe

AB:> Честно говоря, я не вижу большой разницы с применением и без применения CLAHE

```{python}
clahe = cv2.createCLAHE(clipLimit =2.0, tileGridSize=(8,8))
```

```{python}
# clahe?
```

```{python}
stack_shape = stack.shape
#stack_roll = np.reshape(stack, (stack_shape[0],-1))
#print("Rolled shape: ", stack_roll.shape)
# #%time img = clahe.apply(stack_roll).reshape(stack.shape)

img_clahe = np.zeros(stack.shape, np.float32)
for k,plane in enumerate(stack):
    img_clahe[k] = clahe.apply(plane)
```

```{python}
wi = napari.view_image(stack, ndisplay=3, scale=(zoom, 1,1), name='raw', colormap='cyan')
wi.add_image(img_clahe, scale=(zoom,1,1), name='CLAHE',colormap='magenta')
wi.add_image(stack.astype(np.float32)-img_clahe.astype(np.float32),  scale=(zoom,1,1),  name='residuals')
```

```{python}
plt.figure()
plt.hist(np.ravel(stack), 100, histtype='step', log=True, label='raw');
plt.hist(np.ravel(img_clahe), 100, histtype='step', log=True, label='CLAHE');
plt.title("Effect of CLAHE on stack histogram")
plt.legend()
```

```{python}
# check if use clahe or not
img = img_clahe if use_clahe else stack
```

```{python}
max_proj = img.max(0)
```

```{python}
domain_mask = ndi.binary_dilation(largest_region(remove_small_objects(max_proj > 0.5*threshold_li(max_proj))), iterations=3)
domain_mask = ndi.binary_closing(domain_mask,iterations=3)
```

```{python}
plt.imshow(max_proj, cmap='gray')
plt.contour(domain_mask, colors=['r'], levels=[0.5])
```

```{python}
img_cropped = np.array([crop_image(plane,domain_mask, margin=10) for plane in img])
```

```{python}
w = napari.view_image(img_cropped)
```

Важный вопрос, как сделать одинаковым масштаб по осям z и xy. Можно downsample XY, можно upsample (by interpolation) Z. Можно комбинировать. В идеале, наверное, XY не трогать, а сделть upsample по Z. 

```{python}
downscale = 2
# %time img_noisy = ndi.zoom(img_cropped.astype(np.float32), (zoom/downscale, 1/downscale, 1/downscale), order=1)
```

```{python}
plt.imshow(img_noisy.max(0), cmap='gray')
```

```{python}
img.shape, img_noisy.shape
```

```{python}
#w = napari.view_image(img_noisy)
```

# Фильтрация изображения

```{python}
def filter_image(image, filter_func):
    threshold = filter_func(image)
    #img_filt = np.where(image > threshold, image, 0)
    binary_clean = remove_small_objects(image >= threshold, 5, connectivity=3)
    return np.where(binary_clean, image, 0)
```

```{python}
threshold_const = lambda x: 187
```

```{python}
img_clear = filter_image(img_noisy, threshold_li)
if HANDY:
    pass
#     img_clear = filter_image(img_noisy, threshold_minimum)
```

```{python}
final_image = img_clear
```

```{python}
final_image.shape
```

```{python}
# НЕ ЗАКРЫВАТЬ!!!
w = napari.view_image(final_image, colormap='gray', ndisplay=3)
```

# Определение центра

```{python}
import itertools as itt
```

```{python}
def percentile_rescale(arr, plow=1, phigh=99.5):
    low, high = np.percentile(arr, (plow, phigh))
    if low == high:
        return np.zeros_like(arr)
    else:
        return np.clip((arr-low)/(high-low), 0, 1)
```

```{python}
def flat_indices(shape):
    idx = np.indices(shape)
    return np.hstack([np.ravel(x_)[:,None] for x_ in idx])
```

```{python}
X1a = flat_indices(final_image.shape)
```

```{python}

```

```{python}
# %time weights_s = percentile_rescale(np.ravel(ndi.gaussian_filter(final_image,5))**2,plow=99.5,phigh=99.99)
```

```{python}
center = tuple(map(int, np.sum(X1a*weights_s[:,None],axis=0)/np.sum(weights_s)))
center
```

# Выделение сомы

```{python}
from skimage.morphology import erosion, dilation
```

```{python}
from skimage.morphology import skeletonize, flood
```

```{python}
from astromorpho import morpho
```

```{python}
#to_erode = final_image.copy() # для array [:] не создает копию
```

```{python}
# # it's hard to define how many iterations of erosion to make
# niters_erosion = int(np.round(6*zoom/downscale))
# for i in tqdm(range(niters_erosion), desc='grayscale erosion'):
#     to_erode = erosion(to_erode)
```

```{python}
#eroded = remove_small_objects(to_erode > 0, 5, connectivity=3)
```

**Альтернативный подход к сегментации сомы**
1. Работаем со сглаженным стеком
2. делаем первичную маску как flood из центра с толерантностью в 10% разницы между максимальным и минимальным значениями в стеке
3. Разрастаем (аналог flood) первичную маску в несколько итераций

```{python}
#soma_mask = largest_region(np.where(dilation(eroded), True, False))
#soma_mask = largest_region(final_image >= np.percentile(final_image, 99))

smooth_stack = ndi.gaussian_filter(final_image, 3)
tol = (smooth_stack.max() - smooth_stack[final_image>0].min())/10

print('tol:',tol)
# %time soma_seed_mask = flood(smooth_stack, center, tolerance=tol)
```

```{python}
# %time soma_mask = morpho.expand_mask(soma_seed_mask, smooth_stack, iterations = 10)
```

```{python}
if verbose:
    w = napari.view_image(final_image, ndisplay=3, opacity=0.5)
    w.add_image(soma_seed_mask, blending='additive', colormap='cyan')
    w.add_image(soma_mask, blending='additive', colormap='magenta')
```

```{python}

```

```{python}
#soma = [tuple(i) for i in idx[soma_mask]]
#soma_shell = 
# #%time soma_all  = morpho.mask2points(soma_mask)
# #%time soma_shell = morpho.inside_boundary_pixels(soma_mask)
# %time soma_shell = get_shell_mask(soma_mask, as_points=True)
```

```{python}
verbose
```

# Выделение толстых отростков

```{python}
# branches = filter_image(img_noisy, threshold_minimum)
# img_branch = branches/branches.max()
```

```{python}
# flood_mask = flood(img_branch > 0, center)
# if verbose:
#     w.add_image(flood_mask, colormap='red', opacity=0.5)
```

```{python}
# img = np.where(flood_mask, img_branch, 0)
# skeleton = skeletonize(img)
# if verbose:
#     w.add_image(skeleton, blending='additive', colormap='cyan')
```

```{python}
#w_branch_mask = skeleton == 255
```

TEST AREA START

```{python}
# def localize_graph_points(G, point_loc, nodes=None, dist=1):
#     if nodes is None:
#         nodes = G.nodes()
#     node_coords = np.array(nodes.keys())
#     node_dists = np.linalg.norm(point_loc-node_coords, axis=-1)
#     suit_points = [tuple(n) for n in node_coords[node_dists < dist]]
#     return suit_points
```

```{python}
# def set_graph_area(G, area_mask, out=True, set_name=None):
#     coords = np.array(G.nodes())
#     node_check = area_mask[coords[:,0], coords[:,1], coords[:,2]]
#     selected_nodes = [tuple(n) for n in coords[node_check]]
   
#     if set_name is not None:
#         nx.set_node_attributes(G, {tuple(n): {'area':set_name} for n in coords[node_check]})
                
#     if out:
#         return selected_nodes
```

Нужна функция для выделения сабграфа (библиотечная делает ридонли). Функция ниже из доков. Требует осмысления и доработки.

```{python}
def create_subgraph(G, nodes):
    # Create a subgraph SG based on a (possibly multigraph) G
    SG = G.__class__()
    SG.add_nodes_from((n, G.nodes[n]) for n in largest_wcc)
    if SG.is_multigraph():
        SG.add_edges_from((n, nbr, key, d)
            for n, nbrs in G.adj.items() if n in largest_wcc
            for nbr, keydict in nbrs.items() if nbr in largest_wcc
            for key, d in keydict.items())
    else:
        SG.add_edges_from((n, nbr, d)
            for n, nbrs in G.adj.items() if n in largest_wcc
            for nbr, d in nbrs.items() if nbr in largest_wcc)
    SG.graph.update(G.graph)
    return SG
```

```{python}
# nodes= G3.nodes()
# c = np.array(nodes)
# sc = soma_mask[c[:,0], c[:,1], c[:,2]]
# soma_nodes = [tuple(n) for n in c[sc]]
# G3.remove_nodes_from(soma_nodes)

```

```{python}
# Gsub2 = G3.subgraph(points_mean)
# new_nodes = {n:n for n in Gsub2.nodes()}
```

```{python}
# Gsub2 = G3.subgraph(soma_nodes)
# new_nodes = {n:n for n in Gsub2.nodes()}
# props = {'weight': 1 - np.array([edgedata["weight"] for _, _, edgedata in Gsub2.edges(data=True)])}
# w.add_shapes(draw_edges(new_nodes, list(Gsub2.edges())), 
#              shape_type='path', 
#              edge_color='weight', 
#              edge_width=0.1, 
#              edge_colormap='inferno', 
#              properties=props)
```

TEST AREA END


# Матрица Гессе

```{python}
#OUT = True
```

```{python}
2**5
```

```{python}
sigmas = 2.0**np.arange(-1, 4, 1)
```

```{python}
sigmas
```

```{python}
HANDY
```

```{python}
qstacks = {}
```

```{python}
sato_coll = {}
Vf_coll = {}
```

```{python}
for sigma in tqdm(sigmas):
    #astro.morpho.sato3d is newer and uses tensorflow (if it's installed)
    #optimally, the two variants of sato3d should be merged
    sato, Vf = astro.morpho.sato3d(final_image, sigma, hessian_variant='gradient_of_smoothed', do_brightness_correction=False, return_vectors=True)
    sato_coll[sigma] = (sato*sigma**2)*(final_image > 0)
    #sato_coll[sigma] = final_image*sato*sigma**2
    #sato_coll[sigma] = sato*(final_image>0)
    Vf_coll[sigma] = Vf[...,0][...,::-1]
```

```{python}
#Vfx = Vf[...,0][...,::-1]
#V = Vfx[..., 0]
#U = Vfx[..., 1]
#C = Vfx[..., 2]
```

```{python}
sato_coll.keys()
```

```{python}
lengths_coll = {sigma: astro.enh.percentile_rescale(sato)**0.5 for sigma, sato in sato_coll.items()}
```

```{python}
vectors_coll = {}
```

```{python}
for sigma in Vf_coll:
    Vfx = Vf_coll[sigma]
    V = Vfx[..., 0]
    U = Vfx[..., 1]
    C = Vfx[..., 2]
    lengths = lengths_coll[sigma]
    vectors_coll[sigma] = np.stack((U*lengths, V*lengths, C*lengths), axis=3)
```

```{python}

```

```{python}
#sato_best = np.argmax([sato_coll[sigma] for sigma in sigmas], axis=0)
```

```{python}

```

```{python}
from ucats import masks as umasks
```

```{python}

```

```{python}
ksigma = len(sigmas)-1
largest_sigma = sigmas[ksigma]
largest_sigma
```

```{python}
#largest_sigma_seed = largest_region(umasks.select_overlapping(sato_best == ksigma, soma_mask))
sato = sato_coll[sigmas[1]]#*(final_image)
#tol = (sato.max()-sato.min())/5
#print('tol:', tol)
#largest_sigma_seed2 =  largest_region(sato >= 0.5*np.mean(sato[soma_mask]))
threshold = threshold_li(sato[sato>0])
mask = remove_small_objects(sato>threshold, int(sigma*64))
#largest_sigma_seed = flood(sato, center, tolerance=tol)
```

```{python}
0.5**0.5
```

```{python}
masks = {}
for sigma in tqdm(sigmas):
    sato = sato_coll[sigma]
    threshold = threshold_li(sato[sato>0])*sigma**0.5
    print(sigma, threshold)
    masks[sigma] = remove_small_objects(sato > threshold, min_size=int(sigma*64))
```

```{python}
masks[sigmas[-1]] = umasks.select_overlapping(masks[sigmas[-1]], soma_mask)
```

```{python}
for k in range(len(sigmas)-2,-1,-1):
    sigma = sigmas[k]
    masks[sigma] = umasks.select_overlapping(masks[sigma], ndi.binary_dilation(masks[sigmas[k+1]], iterations=5))
```

```{python}
w = napari.view_image(final_image, )
for sigma in masks:
    w.add_image(masks[sigma], blending='additive', name=f'σ={sigma:02f}')
#w.add_image(masks[sigmas[-1]],colormap='blue',blending='additive')
#w.add_image(masks[sigmas[-2]],colormap='red',blending='additive')
#w.add_image(masks[sigmas[-3]],colormap='green',blending='additive')
#w.add_image(sato_best==len(sigmas)-2,colormap='red',blending='additive')
#w.add_image(sato_best==len(sigmas)-3,colormap='green',blending='additive')
#w.add_image(sato_best==len(sigmas)-4,colormap='cyan',blending='additive')
#w.add_image(mask, colormap='red', opacity=0.75)
```

```{python}
vectors_best = np.zeros(vectors_coll[sigmas[0]].shape)
mask_sum = np.zeros(final_image.shape,bool)
masks_exclusive = {}

for k in range(len(sigmas)-1,-1,-1):
    sigma = sigmas[k]
    mask = masks[sigma]
    if k < len(sigmas)-1:
        mask = mask & (mask ^ mask_sum)
    mask_sum += mask.astype(bool)
    masks_exclusive[sigma] = mask
    vectors_best[mask] = vectors_coll[sigma][mask]
```

```{python}
w = napari.view_image(final_image, )
colors = ['red', 'green', 'magenta', 'cyan', 'blue']
for sigma, color in zip(masks, itt.cycle(colors)):
    w.add_image(masks_exclusive[sigma], blending='additive', name=f'σ={sigma:02f}',colormap=color)
```

# Построение графа


Можно предложить как минимум, два варианта объединения масштабов:
 1. [ ] "Best" -- это где вектора в каждом вокселе взяты из соответствующих масок для разных масштабов, потом все это сведено в один граф, и во всем графе
         ищется путь до поверхности сомы. **NOTE:** по идее, маски должны быть "исключительными", то есть каждая область может принадлежать только одной сигме.
 2. [ ] "Combined" -- скелет и пути задаются итеративно от больших масштабов к маленьким, то есть используется свой граф для каждого масштаба и пути ищутся в дополнение к уже найденым. 
       Кстати, можно сделать лучше (предположительно), если вектора из qstack_mask старшего масштаба добавлять к графу меньшего масштаба и опять искать пути до сомы. Тогда будут дополнительно 
       "тренироваться" пути вдоль больших веток. 
       Потом можно брать просто сумму qstacks для разных масштабов, маску можно брать как объединение всех масок на разных уровнях или снова как надпороговые пиксели. 

```{python}
def prep_crops():
    "makes list of crops for edges"
    num2slice = {1: (slice(1,None), slice(None,-1)), 
                 0: (slice(None), slice(None)), 
                -1: (slice(None,-1), slice(1,None))}
    shifts = list(itt.product(*[(-1,0,1)]*3))
    # we only need one half of that
    cut = int(np.ceil(len(shifts)/2))
    crops_new = [list(zip(*[num2slice[n] for n in tuple])) for tuple in shifts[cut:]]
    return crops_new
```

```{python}

```

```{python}
crops_new = prep_crops()
```

```{python}
graph_coll = {sigma:nx.Graph() for sigma in sigmas}
nodes_coll = {sigma:{} for sigma in sigmas}
```

```{python}
graph_coll['best'] = nx.Graph()
nodes_coll['best'] = {}
```

```{python}

```

```{python}
i, j, k = np.indices(final_image.shape)
idx = np.stack((i,j,k), axis=3)
idx.shape
```

```{python}
for sigma in sigmas:
    vectors = vectors_coll[sigma]#*(largest_sigma_seed[...,None])
    vectors = vectors*masks[sigma][...,None]
    for crop, acrop in tqdm(crops_new):
         graph_coll[sigma].add_weighted_edges_from(calc_edges(vectors[crop], vectors[acrop], idx[crop], idx[acrop]))
```

```{python}
key='best'
vectors = vectors_best
graph_coll[key] = nx.Graph()
for crop, acrop in tqdm(crops_new):
         graph_coll[key].add_weighted_edges_from(calc_edges(vectors[crop], vectors[acrop], idx[crop], idx[acrop]))
```

## Добавление точек сомы в граф
Может быть, нам только оболочку сомы добавлять?

```{python}
def get_mask_vals(idxs, mask):
    idx_mask = mask[idxs[:,0], idxs[:,1], idxs[:,2]]
    return idxs[idx_mask]
```

```{python}
def get_edges(mask, index1, index2, weight):
    idx1 = [tuple(i) for i in get_mask_vals(index1.reshape((-1, index1.shape[-1])), mask)]
    idx2 = [tuple(i) for i in get_mask_vals(index2.reshape((-1, index2.shape[-1])), mask)]
    return zip(idx1, idx2, np.full(len(idx1), weight))
```

```{python}

```

```{python}
Gsoma = nx.Graph()
```

```{python}
soma_shell_mask = get_shell_mask(soma_mask)
```

```{python}
wx = napari.view_image(soma_mask)
wx.add_image(soma_shell_mask)
```

```{python}
for crop, acrop in tqdm(crops_new):
    Gsoma.add_weighted_edges_from(get_edges(soma_shell_mask, idx[crop], idx[acrop], 0.7))
```

```{python}
graph_coll
```

```{python tags=c()}
# %%time 

#G = graph_coll[largest_sigma]
#G = graph_coll[current_sigma]

for G in graph_coll.values():
    for p1, p2, weight in Gsoma.edges(data=True):
        try:
            old_weight = G.get_edge_data(p1, p2)['weight']
        except Exception as exc:
            old_weight = 1
        G.add_edge(p1, p2, weight=min(weight['weight'], old_weight))
```

```{python}
nodes_coll = {key:{n:n for n in G.nodes()} for key, G in graph_coll.items()} # just a copy of G3 nodes
```

```{python}
# %%time 

# fix negative weights

for G in graph_coll.values():
    for p1,p2, weight in tqdm(G.edges(data=True)):
        if weight['weight'] < 0:
            print(p1,p2,weight)
            G.add_edge(p1,p2, weight=0)
```

```{python}
# #nx.multi_source_dijkstra_path?
```

```{python}
#center, center in nodes3
```

```{python}
# #nx.single_source_dijkstra_path?
```

```{python}
# current_sigma = sigmas[-1]
# G = graph_coll[current_sigma]
# print(current_sigma)
```

```{python}
# #%time paths_dict = nx.single_source_dijkstra_path(G3, center)
# #%time paths_dict = nx.multi_source_dijkstra_path(G, soma_shell)
```

```{python}
#paths_dict.keys()
```

```{python}
#paths = list(paths_dict.values())
```

```{python}

```

```{python}
#'{}_sigma{}_paths_all'.format(os.path.basename(filename), sigma)
```

```{python}
OUT
```

```{python}
# if OUT:
#     np.save('{}_sigma{}_paths_all'.format(os.path.basename(filename), sigma), np.array(paths, dtype=object))
```

# Расчет путей, встречаемости точек в путях и слияние графов по путям

```{python}
from copy import copy

def find_paths(G, targets, min_count=1, min_path_length=10):
    paths_dict = nx.multi_source_dijkstra_path(G, targets, )
    
    #reverse order of points in paths, so that they start at tips 
    paths_dict = {path[-1]:path[::-1] for path in paths_dict.values() if len(path) >= min_path_length}
    paths = list(paths_dict.values())
    points = count_points_paths(paths)

    qstack = np.zeros(vectors.shape[:-1])  #Это встречаемость точек в путях
    for p, val in points.items():
        if val >= min_count:
            qstack[p] = np.log(val)
    return qstack, paths_dict
```

```{python}

```

```{python}
#qstacks  = {sigma:find_paths(graph_coll[sigma], soma_shell)[0] for sigma in tqdm(graph_coll)}
```

```{python}
qstack_masks = {}
```

```{python}
sigmas
```

## Building all paths at once, using the "best-scale" full graph

```{python}
# %time qstacks['best'], paths_best = find_paths(graph_coll['best'], soma_shell)
qstack_masks['best'] = qstacks['best'] > threshold_li(qstacks['best'][qstacks['best']>0])
```

```{python}
#paths_best_tails = {p[-1]:p for p in tqdm(paths_best.values()) if len(p) >= min_length} #paths, indexed by endpoints ("tips")
```

```{python}
#all_keys = list(paths_best.keys())
all_tips = list(paths_best.keys())
```

```{python}
len(all_tips)
```

```{python}
k = np.random.randint(len(all_tips))

p = paths_best[all_tips[k]]
all_tips[k], p[0], p[-1]
```

```{python}
pa = np.array(p)
```

```{python}
pa[0]
```

```{python}
near_paths = [path for tail, path in paths_best.items() if eu_dist(p[-1],tail) < 10]
```

```{python}

```

```{python}

```

```{python}
len(near_paths)
```

```{python}
#t = w.add_points(pa,size=1, face_color='magenta', edge_color='magenta')
```

```{python}
w = napari.view_image(final_image)
```

```{python}
# Нужно вручную выбрать точки
inits = [tuple(t) for t in w.layers[1].data.astype(int)]
```

```{python}
paths = []

ata = np.array(all_tips)

for t in inits:
    tx = tuple(ata[np.argmin(np.linalg.norm(t-ata, axis=1))])
    paths.append(np.array(paths_best[tx]))
    #np.array(paths_best_tails[t]) for t in inits if t in paths_best_tails
```

```{python}

```

```{python}
[tuple(p[-1]) for p in paths]
```

```{python}
#px = w.add_shapes(pa,shape_type='path', edge_width=1, edge_color='magenta', blending='translucent_no_depth')
```

```{python}
#px2 = w.add_shapes(np.array(near_paths[np.random.randint(len(near_paths))]), shape_type='path', edge_width=0.5, edge_color='green')
```

```{python}
kwargs = dict(edge_width=0.5, shape_type='path', blending='translucent_no_depth')
```

```{python}
pacc = []
for p in paths:
    pacc.append(w.add_shapes(p,   edge_color=np.random.rand(3),  **kwargs))
```

```{python}
domain_shell = get_shell_mask(masks_exclusive[0.5], do_skeletonize=True, as_points=True)
```

```{python}
domain_shell = set(domain_shell) 
```

```{python}
tips = [t for t in tqdm(all_tips) if t in domain_shell]
```

```{python}
len(tips), len(domain_shell)
```

```{python}
tip_paths = [np.array(paths_best[t]) for t in tips]
```

## Converting paths to directed graphs, merging and visualizing the graphs

```{python}
def path_to_graph(path):
    "Converts an ordered list of points (path) into a directed graph"
    g = nx.DiGraph()
    tp = tip_paths[0]
    
    for k,p in enumerate(path):
        tp = tuple(p)
        g.add_node(tp) 
        if k > 0:
            g.add_edge(tp, tuple(path[k-1]), weight=1)
    return g
    
    
def view_graph(g, viewer, color=None, kind='points', name=None):
    if color is None:
        color = np.random.rand(3)
    pts = np.array(g.nodes)
    
    kw = dict(face_color=color, edge_color=color, blending='translucent_no_depth', name=name)
    if kind == 'points':
        viewer.add_points(pts, size=1, **kw)
    elif kind == 'path':
        viewer.add_shapes(pts, edge_width=0.5, shape_type='path', **kw)
        
    

def get_tips(g):
    return {n for n in g.nodes if len(list(g.successors(n))) == 0}
            
def get_roots(g):
    return {n for n in g.nodes if len(list(g.predecessors(n))) < 1}

def get_branch_points(g):
    return {n for n in gx.nodes if len(list(gx.successors(n))) > 1}            
```

```{python}
graphs = [path_to_graph(tp) for tp in tqdm(tip_paths)]
```

```{python}
near_graphs = [path_to_graph(tp) for tp in near_paths]
```

```{python}
picked_graphs = [path_to_graph(tp) for tp in paths]
```

```{python}
g1,g2 = near_graphs[0], near_graphs[101]
view_graph(g1, w, kind='path', name='1')
view_graph(g2, w, kind='path', name='2')
```

```{python}

```

```{python}
# #%time gx = nx.compose_all(near_graphs + [graphs[100]])
# %time gx = nx.compose_all(picked_graphs)
```

```{python}
roots = get_roots(gx)
roots
```

```{python}
tips = get_tips(gx)
tips
```

```{python}
def graph_to_paths(g, min_path_length=1):
    """
    given a directed graph, return a list of a lists of nodes, collected
    as unbranched segments of the graph
    """

    roots = get_roots(g)
    
    def _acc_segment(root, segm, accx):
        if segm is None:
            segm = []
        if accx is None:
            accx = []
        children = list(g.successors(root))
        
        if len(children) < 1:
            accx.append(segm)
            return
        
        elif len(children) == 1:
            c = children[0]
            segm.append(c)
            _acc_segment(c, segm, accx)
        
        if len(children) > 1:
            #segm.append(root)
            accx.append(segm)
            for c in children:
                _acc_segment(c, [root, c], accx)        
    
    acc = {}
    for root in roots:
        px = []
        _acc_segment(root, [], px)
        acc[root] = [s for s in px if len(s) >= min_path_length]
    return acc
            
        
        
def paths_to_colored_stack(paths, shape, change_color_at_branchpoints=False):
    #colors = np.random.randint(0,255,size=(len(paths),3))
    stack = np.zeros(shape + (3,), np.uint8)
    for root in paths:
        color =  np.random.randint(0,255, size=3)
        for kc,pc in enumerate(paths[root]):
            if change_color_at_branchpoints:
                color = np.random.randint(0,255, size=3)
            for k,p in enumerate(pc):
                #print(k, p)
                stack[tuple(p)] = color
    return stack
```

```{python}

```

```{python}
xpaths = graph_to_paths(gx, break_at_branchpoints=True)
len(xpaths)
```

```{python}
len(xpaths[(109, 249, 208)])
```

```{python}
# %time colored_paths = paths_to_colored_stack(xpaths, final_image.shape, change_color_at_branchpoints=True)
```

```{python}
w.add_image(colored_paths,  channel_axis=3, colormap=['red','green','blue'], name='colored_paths')
```

```{python}

```

```{python}
# %time gx_all = nx.compose_all(graphs) # can take a while
```

```{python}
# %time xpaths_all = graph_to_paths(gx_all)
```

```{python tags=c()}
# %time colored_paths_all = paths_to_colored_stack(xpaths_all, final_image.shape, change_color_at_branchpoints=False)
```

```{python}
#props = {'path-id': ['line'+str(i) for i in np.arange(len(xpaths))]}
w.add_image(colored_paths_all,  channel_axis=3, colormap=['red','green','blue'], name='cp_all')
```

## Gradual building of the paths from different scales

```{python}
qstacks = {}
qstack_masks = {}
path_dicts = {}

for ksigma in range(len(sigmas)-1,-1,-1):
    #alternative: use shell of a full mask as target
    sigma = sigmas[ksigma]
    print(ksigma, sigma)
    G = graph_coll[sigma]
    
    if ksigma == len(sigmas)-1:
        targets = soma_shell
    else:
        higher_sigma = sigmas[ksigma+1]        
        #targets = astro.morpho.boundary_pixels(masks[higher_sigma])
        targets = astro.morpho.mask2points(qstack_masks[higher_sigma])
        targets = [t for t in targets if G.has_node(t)]
        if not len(targets):
            print('No points from targets in the graph')
            continue
    qstack, path_dict = find_paths(G, targets)
    qstack_mask = qstack > threshold_li(qstack[qstack>0])
    if not np.any(qstack_mask):
        print('empty qstack mask!')
        if ksigma < len(sigmas)-1:
            qstack_mask = qstack_masks[higher_sigma]
    qstacks[sigma] = qstack
    qstack_masks[sigma] = qstack_mask
    path_dicts[sigma] = path_dict
```

### ...and trying to merge everythin to graphs. Fails at the moment (:AB)

```{python}
#path_dicts[0.5].values()
```

```{python}
# %%time 

sigma_pgraphs = []
for pd in path_dicts.values():
    for path in pd.values():
        sigma_pgraphs.append(path_to_graph(path))
```

```{python}
len(sigma_pgraphs)
```

```{python}
# %time merged_pgraphs = nx.compose_all(sigma_pgraphs) # can take a while
```

```{python}
sys.getrecursionlimit()
```

```{python}
#sys.setrecursionlimit(100000)
```

```{python}
roots = get_roots(merged_pgraphs)

```

```{python}
# %time xpaths = graph_to_paths(merged_pgraphs)
```

```{python}

```

```{python}
# for ksigma in range(len(sigmas)-2,0,-1):
#     #alternative: use shell of a full mask as target
#     sigma = sigmas[ksigma]
#     print(ksigma, sigma)
#     G = graph_coll[sigma]
#     targets = astro.morpho.mask2points(qstack_masks[sigmas[ksigma+1]])
#     targets = [t for t in targets if G.has_node(t)]
#     if not len(targets):
#         print('No points from targets in the graph')
#         targets = astro.morpho.boundary_pixels(masks[sigmas[ksigma+1]])
#         targets = [t for t in targets if G.has_node(t)]
#         if not len(targets):
#             print("Couldn't fix this")
#             continue
#     qstack = find_paths(G, targets)[0]
#     qstack_mask = qstack > threshold_li(qstack[qstack>0])
#     if not np.any(qstack_mask):
#         print('empty qstack mask!')
#         qstack_mask = qstack_masks[sigmas[ksigma+1]]
#     qstacks[sigma] = qstack
#     qstack_masks[sigma] = qstack_mask
```

```{python}
sigmas
```

```{python}

```

### Qstack-based approachs


```{python}
qstacks['combined'] = sum(qstacks[sigma] for sigma in sigmas)
qstack_masks['combined'] = sum(qstack_masks[sigma] for sigma in sigmas) > 0
```

```{python}
#sigma = sigmas[ksigma]
#sigma = largest_sigma

if verbose:
    #w.add_image(qstack)
    w = napari.view_image(final_image, ndisplay=3, opacity=0.5)
    w.add_image(soma_mask, name='soma', interpolation='nearest', blending='additive',colormap='cyan')
    #w.add_image(sato_coll[current_sigma], name=f'sato@{current_sigma:0.2f}', colormap='inferno', blending='additive')
    #w.add_image(largest_sigma_seed.astype(np.float32), name=f'largest_sigma_seed', interpolation='nearest', colormap='blue', blending='additive')
    for sigma in qstacks:
        name = sigma if isinstance(sigma, str) else f'qstack@{sigma:0.2f}'
        qstack = qstacks[sigma]
        #data_range = np.percentile(qstack[qstack>0], (1,99))
        w.add_image(qstack, name=name, interpolation='nearest', blending='additive', contrast_limits=data_range, visible=False)
        #threshold = threshold_li(qstack[qstack>0])
        w.add_image(qstack_masks[sigma], name=name+':qmask', blending='additive', colormap='red')
        if isinstance(sigma, (int, float)):
            w.add_image(masks[sigma], name=f'mask@{sigma:0.2f}', blending='additive', colormap='green',visible=False)
    #w.add_image(qstacks[sigma], name=f'qstack@{sigma:0.2f}', interpolation='nearest', colormap='magenta', blending='additive',contrast_limits=(0,12))
    #w.add_image(sato_best == len(sigmas)-1, name='kbest', colormap='red',blending='additive')
    #w.add_image(qstacks[sigma]*largest_sigma_seed, name=f'x@{sigma:0.2f}', interpolation='nearest', colormap='green', blending='additive', contrast_limits=(0,12))
        
```

```{python}
combined_mask = sum(qstack_masks[sigma] for sigma in qstack_masks) > 0
```

```{python}
combined_mask = remove_small_objects(combined_mask, 4)
```

```{python}
combined_skeleton = skeletonize(combined_mask)
```

```{python}
w = napari.view_image(final_image, ndisplay=3, opacity=0.5)
w.add_image(combined_mask, colormap='green', blending='additive')
w.add_image(combined_skeleton, colormap='red', blending='additive')
```

```{python}
#qstacks[largest_sigma]
```

```{python}
qstacks[sigmas[-2]] = find_paths(graph_coll[sigmas[-2]], astro.morpho.mask2points(qstacks[sigmas[-1]]>3.5))[0]
```

```{python}

```


## Ниже ничего особо не менялось

```{python}
if OUT:
    np.save(f'{os.path.basename(filename)}-qstacks.npy',qstacks)
```

```{python}

```

```{python}

```

```{python}
qstack_mask = qstack > 1
```

```{python}
#napari.view_image(skeletonize(qstack_mask))
# = napari.view_image(final_image, ndisplay=3)
#w.add_image(qstack_mask)
```

```{python}
p = np.asarray(list(points_count.keys()))
```

```{python}
vals = np.asarray(list(points_count.values()))
```

```{python}
plt.figure(figsize=(16,8))
plt.hist(np.log(vals[vals>1]), bins=500)
#plt.xlim(-10, 100)
plt.axvline(np.mean(np.log(vals)), color='cyan', ls='--', label='th.mean')
plt.axvline(threshold_li(np.log(vals)), color='tomato', ls='--', label='th.Li')
plt.axvline(np.log(np.mean(vals)), color='b', ls='--', label='th.log_mean')
plt.legend()
plt.show()
```

```{python}
plt.figure(figsize=(16,8))
plt.hist(vals[vals>1], bins=500)
#plt.xlim(-10, 100)
plt.axvline(np.mean(vals), color='b', ls='--', label='th.mean')
plt.axvline(threshold_li(vals), color='tomato', ls='--', label='th.Li')
plt.axvline(np.exp(np.mean(np.log(vals))), color='cyan', ls='--', label='th.log_mean')
plt.legend()
plt.show()
```

```{python}
points_mean = [k for k, v in points_count.items() if v > np.mean(vals)]
thr_li = threshold_li(vals)
points_li = [k for k, v in points_count.items() if v > thr_li]
# points_log_mean = [k for k, v in points_count.items() if np.log(v) > np.mean(np.log(vals))]
# points_log_li = [k for k, v in points_count.items() if np.log(v) > threshold_li(np.log(vals))]
```

```{python}
if verbose:
    w.add_points(draw_nodes(nodes3, points_mean), size=1, edge_color='transparent', face_color='c', name='filt mean')
    w.add_points(draw_nodes(nodes3, points_li), size=1, edge_color='transparent', face_color='g', name='filt li')
```

# Выделение тонких веток

```{python}
Gsub = G3.subgraph(points_mean)
nodes_sub = {n:n for n in Gsub.nodes()}
```

```{python}
props = {'weight': 1 - np.array([edgedata["weight"] for _, _, edgedata in Gsub.edges(data=True)])}
```

```{python}
w.add_shapes(draw_edges(nodes_sub, list(Gsub.edges())), shape_type='path', edge_color='weight', edge_width=0.1, edge_colormap='inferno', properties=props)
```

```{python}

c = np.array(Gsub.nodes())
```

```{python}
graph_mask = np.full(final_image.shape, False)
graph_mask[c[:,0], c[:,1], c[:,2]] = True
```

```{python}
th_branch_mask = np.full(final_image.shape, False)
th_branch_mask[c[:,0], c[:,1], c[:,2]] = True
th_branch_mask[flood_mask] = False
```

```{python}
th_branch_mask = remove_small_objects(th_branch_mask, 5, connectivity=3)
```

# Объединение

```{python}
branch_mask = np.full(final_image.shape, False)
branch_mask[th_branch_mask] = True
branch_mask[w_branch_mask] = True
branch_mask[soma_mask] = True

if verbose:
    bm = napari.view_image(final_image, colormap='magenta', ndisplay=3)
    bm.add_image(branch_mask, blending='additive', colormap='cyan')
```

file:///home/levtg/astro-morpho/data/pictures_nb/join%20plan.jpg![image.png](attachment:image.png)


## Выделение крайних точек

```{python}
from skimage.morphology import cube
edge = dilation(flood_mask, cube(3)) ^ flood_mask
points = edge & th_branch_mask
```

```{python}
if verbose:
    bm.add_image(points, blending='additive', colormap='red', gamma=0.2)
```

```{python}
if verbose:
    bm.add_image(w_branch_mask, blending='additive')
    bm.add_image(th_branch_mask, blending='additive')
    bm.add_image(soma_mask, blending='additive')
    bm.add_image(flood_mask, opacity=0.5, colormap='red')
```

## Ближайшие точки скелета

```{python}
points_coords = idx[points]
skelet_coords = idx[w_branch_mask]
```

```{python}
points2connect = {}
for point in points_coords:
    point_dists = np.linalg.norm(point - skelet_coords, axis=-1)
    if len(points2connect.keys()) > 0:
        point_n_dists = np.linalg.norm(point - np.array(list(points2connect.keys())), axis=-1)
        if point_n_dists.min() < 1.5:
            continue
    if point_dists.min() > 1.5:
        coord = skelet_coords[point_dists.argmin()]
        points2connect[tuple(point)] = tuple(coord)
```

```{python}
print(points_coords.shape, len(points2connect.keys()))
```

## Добавление соединений

```{python}
# from networkx.algorithms.shortest_paths.weighted import multi_source_dijkstra
full_mask = branch_mask.copy()
for s, e in points2connect.items():
#     d, p = multi_source_dijkstra(G3, [tuple(n) for n in skelet_coords], s, 'weight')
    path = nx.shortest_path(G3, s, e, 'weight')
    path_coords = np.array(path)
    full_mask[path_coords[:,0], path_coords[:, 1], path_coords[:,2]] = True
```

```{python}
bm.add_image(full_mask)
```

# Многомасштабный гессиан

```{python}
from skimage.filters import ridges as skridges
```

![image.png](attachment:image.png)


https://content.iospress.com/download/bio-medical-materials-and-engineering/bme1149?id=bio-medical-materials-and-engineering%2Fbme1149


```{python}
def vesselness(img, sigma, alpha= 0.5, beta=0.5, cc=20):
    #H = skf.hessian_matrix(img,sigma)
    #Hl = skf.hessian_matrix_eigvals(H)
    Hl = skridges.compute_hessian_eigenvalues(img, sigma)
    sh = img.shape
    v = np.zeros(sh)
    for r in range(sh[0]):
        for c in range(sh[1]):
            for h in range(sh[2]):
                arr = (Hl[0][r,c,h], Hl[1][r,c,h], Hl[2][r,c,h])

                l1,l2,l3 = sorted(arr, key=abs)
                
                if l2 >= 0 or l3 >=0:
                    v[r,c,h] = 0
                else:
                    Ra = np.abs(l2/l3)
                    Rb = np.abs(l1/np.sqrt(np.abs(l2*l3)))
                    S = np.sqrt(l1**2 + l2**2 + l3**2)
                    v[r,c, h] =  (1 - np.exp(-Ra**2/(2*alpha**2)))*np.exp(-Rb**2/(2*beta**2))*(1 - np.exp(-S**2/(2*cc**2)))
    return v


def multiscale_vesselness(img, sigma_start, sigma_stop, nlevels=50, reduce_fn=np.max):
    return reduce_fn([vesselness(img, sigma)*sigma**2 for sigma in np.linspace(sigma_start,sigma_stop, nlevels)],0)

```

```{python}
def multiscale_sigma(img, sigma_start, sigma_stop, nlevels=50):
    sout = np.zeros(img.shape)
    hout = np.zeros(img.shape)
    vout = np.zeros((*img.shape, 3, 3))
    for sigma in tqdm(np.linspace(sigma_start, sigma_stop, nlevels)):
        hcurr, vcurr = astro.hessian.sato3d(img, sigma, hessian_variant='gradient_of_smoothed', return_vectors=True)
        hcurr *= sigma**2
        mask = hcurr > hout
        
        hout[mask] = hcurr[mask]
        sout[mask] = sigma
        vout[mask] = vcurr[mask]
    return hout, sout, vout
```

```{python}
h, s, v = multiscale_sigma(final_image, 0.5, 4)
```

```{python}
hs = napari.view_image(s, ndisplay=3)
hs.add_image(h)
```

```{python}
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
colors = ["black", ""]
cmap1 = LinearSegmentedColormap.from_list("mycmap", colors)
```

```{python}
hs.add_image(np.where(full_mask^soma_mask, s, 0))
```

```{python}
hs.add_image(final_image)
```

```{python}
from hessian_vectors import add_hessian_vectors, sato2napari_vectors
```

```{python}
add_hessian_vectors(hs, *sato2napari_vectors(h, v))
```

```{python}
v_n, h_n = sato2napari_vectors(h, v)
v_n.shape


```

```{python}
add_hessian_vectors(hs, v_n.reshape((*full_mask.shape,2,3))[full_mask], h_n.reshape(full_mask.shape)[full_mask])
```

```{python}
sigma_skel = np.where(full_mask, s, 0)
hs.add_image(np.where(sigma_skel < 1, sigma_skel, 0), blending='additive', name='0.5-1')
hs.add_image(np.where(np.where(sigma_skel > 1, sigma_skel, 0) < 2, sigma_skel, 0), blending='additive', name='1-2')
hs.add_image(np.where(np.where(sigma_skel > 2, sigma_skel, 0) < 3, sigma_skel, 0), blending='additive', name='2-3')
hs.add_image(np.where(sigma_skel > 3, sigma_skel, 0), blending='additive', name='3-4')
```

```{python}
w.add_image(full_mask)
```

```{python}

```

```{python}

```
